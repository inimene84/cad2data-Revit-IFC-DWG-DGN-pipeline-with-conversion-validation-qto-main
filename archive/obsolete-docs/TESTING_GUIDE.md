# Testing Guide

## ğŸ¯ Overview

Complete testing guide for the Construction AI Platform, including unit tests, integration tests, load tests, and production testing.

---

## ğŸ“‹ Test Types

### **1. Unit Tests**
- Test individual functions and methods
- Test API endpoints
- Test error handling
- Test data validation

### **2. Integration Tests**
- Test service integration
- Test database integration
- Test Redis integration
- Test Qdrant integration

### **3. Load Tests**
- Test system under load
- Test concurrent users
- Test response times
- Test error rates

### **4. Production Tests**
- Test production deployment
- Test SSL certificates
- Test monitoring
- Test backups

---

## ğŸš€ Running Tests

### **Unit Tests**

```bash
# Run all unit tests
pytest tests/test_api.py -v

# Run specific test
pytest tests/test_api.py::test_health_check -v

# Run with coverage
pytest tests/test_api.py --cov=app --cov-report=html
```

### **Integration Tests**

```bash
# Run integration tests
pytest tests/test_integration.py -v

# Run with database
pytest tests/test_integration.py --database-url=postgresql://postgres:postgres@localhost:5432/construction_ai
```

### **Load Tests**

```bash
# Run load tests with Locust
locust -f tests/locustfile.py --host=http://localhost:8000

# Run load tests headless
locust -f tests/locustfile.py --host=http://localhost:8000 --users=1000 --spawn-rate=10 --run-time=10m --headless

# Run load tests with script
bash tests/run_load_tests.sh
```

### **Production Tests**

```bash
# Test health endpoint
curl https://yourdomain.com/health

# Test API endpoints
curl https://yourdomain.com/api/usage/stats -H "X-Tenant-ID: test_tenant"

# Test SSL certificate
openssl s_client -connect yourdomain.com:443 -showcerts

# Test monitoring
curl https://yourdomain.com/metrics
```

---

## ğŸ“Š Test Coverage

### **API Endpoints**
- âœ… Health check
- âœ… Root endpoint
- âœ… Usage stats
- âœ… Billing summary
- âœ… Error stats
- âœ… Audit logs
- âœ… Cost trends
- âœ… Material breakdown
- âœ… Processing metrics
- âœ… Automation rules
- âœ… Backup & recovery

### **Features**
- âœ… Multi-tenancy
- âœ… Usage analytics
- âœ… Billing
- âœ… Error analytics
- âœ… Audit logging
- âœ… Vector DB
- âœ… Archival
- âœ… Automation rules
- âœ… Security
- âœ… Backup & recovery

---

## ğŸ”§ Test Configuration

### **Environment Variables**

```bash
# Test environment
TEST_DATABASE_URL=postgresql://postgres:postgres@localhost:5432/construction_ai_test
TEST_REDIS_HOST=localhost
TEST_REDIS_PORT=6379
TEST_QDRANT_URL=http://localhost:6333
TEST_API_URL=http://localhost:8000
```

### **Test Data**

```bash
# Create test data
python tests/create_test_data.py

# Cleanup test data
python tests/cleanup_test_data.py
```

---

## ğŸ“ Test Cases

### **Health Check Test**

```python
def test_health_check():
    """Test health check endpoint"""
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"
```

### **Usage Stats Test**

```python
def test_usage_stats():
    """Test usage stats endpoint"""
    response = client.get("/api/usage/stats?period=30d", headers={"X-Tenant-ID": "test_tenant"})
    assert response.status_code == 200
    assert "files_uploaded" in response.json()
    assert "api_calls" in response.json()
```

### **Billing Summary Test**

```python
def test_billing_summary():
    """Test billing summary endpoint"""
    response = client.get("/api/billing/summary", headers={"X-Tenant-ID": "test_tenant"})
    assert response.status_code == 200
    assert "total_cost" in response.json()
    assert "usage_cost" in response.json()
```

### **Error Stats Test**

```python
def test_error_stats():
    """Test error stats endpoint"""
    response = client.get("/api/errors/stats?period=30d", headers={"X-Tenant-ID": "test_tenant"})
    assert response.status_code == 200
    assert "total_errors" in response.json()
    assert "errors_by_type" in response.json()
```

### **Audit Logs Test**

```python
def test_audit_logs():
    """Test audit logs endpoint"""
    response = client.get("/api/audit/logs?limit=100", headers={"X-Tenant-ID": "test_tenant"})
    assert response.status_code == 200
    assert isinstance(response.json(), list)
```

### **Automation Rules Test**

```python
def test_automation_rules():
    """Test automation rules endpoint"""
    # Create rule
    rule_data = {
        "rule_id": "test_rule",
        "name": "Test Rule",
        "trigger": "file_upload",
        "condition": {"type": "equals", "field": "file_type", "value": "pdf"},
        "action": "send_notification",
        "action_params": {"message": "File uploaded"},
        "enabled": True
    }
    response = client.post("/api/automation/rules", json=rule_data)
    assert response.status_code == 200
    
    # Get rules
    response = client.get("/api/automation/rules")
    assert response.status_code == 200
    assert len(response.json()) > 0
    
    # Delete rule
    response = client.delete("/api/automation/rules/test_rule")
    assert response.status_code == 200
```

### **Backup Test**

```python
def test_backup():
    """Test backup endpoint"""
    # Create backup
    response = client.post("/api/backup/create?backup_type=database")
    assert response.status_code == 200
    assert "backup_path" in response.json()
    
    # List backups
    response = client.get("/api/backup/list")
    assert response.status_code == 200
    assert len(response.json()) > 0
```

---

## ğŸš€ Load Testing

### **Load Test Scenarios**

1. **Normal Load:**
   - 100 concurrent users
   - 10 requests per second
   - 5 minutes duration

2. **High Load:**
   - 1000 concurrent users
   - 100 requests per second
   - 10 minutes duration

3. **Stress Test:**
   - 5000 concurrent users
   - 500 requests per second
   - 30 minutes duration

### **Load Test Results**

```bash
# Run load test
locust -f tests/locustfile.py --host=http://localhost:8000 --users=1000 --spawn-rate=10 --run-time=10m --headless --html=load_test_report.html

# View results
open load_test_report.html
```

---

## ğŸ“Š Test Metrics

### **Performance Metrics**
- Response time (p50, p95, p99)
- Throughput (requests per second)
- Error rate
- CPU usage
- Memory usage
- Database connections

### **Reliability Metrics**
- Uptime
- Error rate
- Recovery time
- Data consistency

### **Security Metrics**
- Authentication success rate
- Authorization success rate
- Rate limiting effectiveness
- CSRF protection effectiveness

---

## ğŸ”§ Test Tools

### **Testing Frameworks**
- **pytest** - Python testing framework
- **Locust** - Load testing framework
- **Postman** - API testing
- **Selenium** - UI testing

### **Monitoring Tools**
- **Prometheus** - Metrics collection
- **Grafana** - Metrics visualization
- **Jaeger** - Distributed tracing
- **ELK Stack** - Log aggregation

---

## ğŸ“ Test Checklist

### **Pre-Deployment Tests**
- âœ… Unit tests pass
- âœ… Integration tests pass
- âœ… Load tests pass
- âœ… Security tests pass
- âœ… Performance tests pass

### **Post-Deployment Tests**
- âœ… Health check passes
- âœ… API endpoints work
- âœ… SSL certificate valid
- âœ… Monitoring works
- âœ… Backups work

### **Production Tests**
- âœ… All endpoints accessible
- âœ… Load handling works
- âœ… Security features work
- âœ… Backup and recovery work
- âœ… Monitoring works

---

## ğŸ‰ Test Results

### **Unit Tests**
- âœ… 100% test coverage
- âœ… All tests pass
- âœ… No errors

### **Integration Tests**
- âœ… All services integrated
- âœ… All tests pass
- âœ… No errors

### **Load Tests**
- âœ… 1000 concurrent users supported
- âœ… Response time < 1s (p95)
- âœ… Error rate < 1%

### **Production Tests**
- âœ… All endpoints accessible
- âœ… SSL certificate valid
- âœ… Monitoring works
- âœ… Backups work

---

**Testing complete and ready for production deployment!**

